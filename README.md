Swagger: http://localhost:7777/swagger-ui/index.html

Docker Commands:

      1. check docker live or not: docker info
      2. docker build -t current-usa-time .
      3. docker run -p dockerPort:CodePort current-usa-time
      4. docker pull seturini/current-usa-time
      5. docker tag current-usa-time seturini/current-usa-time:latest
      6. docker push seturini/current-usa-time:latest
      7. docker ps (list down all active containers)
      8. docker ps -a (list down all containers)
      9. docker stop ContainerId
      10. docker-compose.yml used to define and configure multiple containers
      11. docker run - to create and start container
      12. docker start - to only start container
      13. docker stop - to stop container
      14. docker rm ContainedId - to remove container
      15. docker-compose.override.yml - to override setting and config of docker-compose.yml
      16. docker logs containerId - To see logs

**Kubernetes:**

      Why we need it ?
      1. Scaling: Automatically scale containers based on demand.
      2. High Availability: Ensure containers are restarted and rescheduled if they fail.
      3. Load Balancing: Distribute traffic across multiple containers.
      4. Self-Healing: Automatically restart or reschedule failed containers.
      5. Service Discovery: Manage networking between containers automatically.
      6. Automated Rollouts: Handle safe deployments and rollbacks.
      7. Resource Management: Allocate CPU/memory to prevent overuse.
      8. Multi-container Coordination: Manage complex applications with multiple containers.
      9. Environment Consistency: Ensure consistent behavior across environments.
      10. Multi-cloud Support: Run containers across different cloud providers.


   check kubernetes live or not: kubectl cluster-info

      1. In Kubernetes, a Pod is the smallest deployable unit that you can create and manage. It's a fundamental concept in Kubernetes and represents a single instance of a running process in your cluster:
      2. Imagine you have a web application that consists of a web server and a caching server. You could deploy these two components in separate containers within the same Pod. This allows them to easily communicate with each other (e.g., the web server can access the cache on localhost) and be managed as a single unit.
      3. kubectl apply -f deployment.yaml   --> Create pods
      4. kubectl apply -f service.yaml  --> A Service routes traffic to pods based on the labels defined in the Service's selector field.
      5. kubectl get deployment
      6. kubectl get service
      7. kubectl get pods
      8. to kill deployment: kubectl delete deployment current-usa-time-deployment ( will kill pods)
      9. to kill service: kubectl delete service current-usa-time-service

      ![img.png](img.png)
      ![img_1.png](img_1.png)

**Kafka Commands**

      1. Start kafka in docker container: docker-compose up -d
      2. To see topics: docker exec -it kafka kafka-topics --bootstrap-server localhost:9092 --list
      3. Create Topic: docker exec -it kafka kafka-topics --bootstrap-server localhost:9092 --create --topic my-topic --partitions 3 --replication-factor 1

      Replication Factor: The number of copies of each partition that are maintained across different brokers in your Kafka cluster, we have only one broker so kept it as 1
      How it works: When you create a topic with a replication factor greater than 1, Kafka distributes the partitions and their replicas across the available brokers. For example, with a replication factor of 3 and 2 partitions, each partition will have 3 copies stored on 3 different brokers (ideally), Each replica contains the exact same data as the other replicas of that partition.. 

      Fault Tolerance: If a broker goes down, the replicas on the other brokers can take over, ensuring that data is still available and consumers can continue to read and write messages.

      Consumer LAG: Group property : Messages in the topic - Messages Consumed

      If there is one partition and two consumer groups, the same message will be read by both consumer groups independently.

      The maximum number of active consumers in a group should be equals to the number of partitions. Extra consumers will sit idle.

      Different Partition will have different data, we can control it too by passing same key, same key data will go to same partition
      Let's say we have 2 partition, one consumer group and 2 consumers setup , then c1 will get p1 data and c2 will get p2 data while if only one consumer present then c1 will get both p1, p2 data

      Kafka Streams used send messaage from one topic to another (KafkaStream.java)

      4. Describe specific topic: docker exec -it kafka kafka-topics --bootstrap-server localhost:9092 --describe --topic my-topic
      5. Delete topic: docker exec -it kafka kafka-topics --bootstrap-server localhost:9092 --delete --topic my-topic
      6. Benefits of Using client-id: 
         Traceability: Makes it easier to identify which Kafka client is performing admin operations when reviewing Kafka logs or metrics.
      7. Publish : curl -X POST "http://localhost:7777/publish?topic=my-topic&message=HelloKafka"
      8. See current offset: docker exec -it kafka kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group my-group
      9. See all messages: docker exec -it kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic my-topic --from-beginning --max-messages 10
         We can reset only if consumers are not active
      10. Reset of earlier offset: docker exec -it kafka kafka-consumer-groups --bootstrap-server localhost:9092 --group my-group --topic my-topic --reset-offsets --to-earliest --execute
      11. Reset to latest offset: docker exec -it kafka kafka-consumer-groups --bootstrap-server localhost:9092 --group my-group --topic my-topic --reset-offsets --to-latest --execute

*** Kafka Interview prep ***

      1. Replication Factor 2 and partition 3 means if any publisher publish data it will be divided into 3 partitions and all 3 partitions data will be stored in kafka1 and kafka2 broker

      2. Replication factor <= No of brokers

      3. Leader vs Follower
         Leader: The broker that handles all reads and writes for a partition. Producers send data here. Consumers also fetch data here.
         Follower: The broker that just replicates from the leader. It doesnâ€™t serve client requests unless the leader fails.
         Partition	Leader	Follower
         P0	        kafka1	kafka2
         P1	        kafka1	kafka2
         P2	        kafka2	kafka1

         What happens in practice:
            Write: Producer sends message to partitionâ€™s leader (say P0 â†’ kafka1).
            Replication: Follower (kafka2) pulls the message from kafka1 and stores it.
            Ack to producer: If acks=all, producer waits until both leader and follower confirm.
            Failover: If kafka1 (leader of P0) goes down â†’ follower kafka2 is promoted to leader.
      
      4. ISR = In-Sync Replicas: Itâ€™s the set of replicas (leader + followers) that are fully caught up with the leaderâ€™s log. 
         How it works:
            Each partition has 1 leader and N-1 followers.
            Followers constantly pull data from the leader.
            If a follower is keeping up (not lagging behind more than a threshold), itâ€™s part of the ISR set otherwise it won't be part of ISR
            Only ISR members can be promoted to leader if the current leader fails.

      5. acks in Kafka = Producer acknowledgment setting
         It controls when the producer considers a message successfully â€œwrittenâ€ (and stops worrying about it).
         acks=0
            Producer doesnâ€™t wait for any acknowledgment.
            Fire and forget â†’ Fastest but unsafe (data may be lost if leader crashes before replication).

         acks=1 (default)
            Producer gets an ack from the leader only.
            Message is considered successful once the leader writes it to its local log.
            Followers may not have replicated yet â†’ If leader dies immediately, data loss is possible.

         acks=all (or acks=-1)
            Producer waits until all in-sync replicas (ISR) confirm the write.
            Safest, but slowest.
            Guarantees no acknowledged message is lost as long as at least one ISR is alive.

      6. retries: 
         Default: retries=2147483647 (basically infinite, in newer Kafka versions)
         In older versions, default was 0 (no retries).
         When retries are enabled:
            Producer sends a message to the leader.
            If leader is unavailable or replication fails â†’ retry happens after some delay.
            Retries stop once the message is acknowledged or retry count is exhausted.
            If all retries fail â†’ exception is thrown back to your app.

      7. Zombie broker and epoch:
         Whatâ€™s a Zombie Broker?
            A zombie broker is a broker that:
               Was a leader for a partition which crashed
               Meanwhile, Kafkaâ€™s controller promoted another replica as the new leader.
               The old broker comes back online but still thinks itâ€™s the leader (because itâ€™s working with stale metadata).

         ðŸ‘‰ Problem: If this zombie accepts writes from producers, those writes are lost (because the real leader + ISR donâ€™t know about them).

         âš¡ How Kafka fixes this â†’ Epochs

         Kafka uses a leader epoch to prevent zombies from corrupting data.
         Leader Epoch: Every time a new leader is elected for a partition, Kafka increments a leader epoch number.
         Producers and followers know which epoch is current.

         How it works:
            Partition P0 has leader on Broker1 with epoch 5.
            Broker1 crashes â†’ Broker2 becomes leader with epoch 6.
            Broker1 comes back online (zombie). It still thinks itâ€™s leader with epoch 5.
            Producers/followers now only accept writes from epoch=6 leader.
            Broker1 (zombie) tries to act like leader â†’ cluster rejects it because its epoch is outdated.
            
            ðŸ‘‰ This ensures only the latest leader can accept writes, killing the zombie problem.

      8. Offset: Property of consumer group tha't why one partiton can have multiple Consumer group and offset will be different in each CG

        Types:
        a. Auto Commit:
                Kafka automatically commits offsets at a fixed interval.
            Controlled by:
                enable.auto.commit=true
                auto.commit.interval.ms=5000  # default 5 sec

            Behavior:
                Consumer polls messages.
                Every 5s (default), Kafka commits the latest offset the consumer has returned.
                If consumer crashes after reading but before processing â†’ offset is already committed = message loss risk (message skipped).
                If processing is super fast â†’ can commit offsets for messages not fully processed = data loss.
                ðŸ‘‰ Good for simple apps where "at-most-once" semantics are okay.

        b. Manual Commit: You control when offsets get committed. Ex: ack.acknowledge()

      9. _consumer_offsets topic: Itâ€™s a Kafka internal compacted topic (created automatically) which stores committed offsets of all consumer groups.
            Store in key value par, where key: (consumer_group, topic, partition) and value: latest committed offset.
            Because itâ€™s a compacted topic, Kafka only keeps the latest offset per key, cleaning up older commits.

            Example:
                Say you have:
                    Topic: orders with partition 0.
                    Consumer group: payment-service.

                Flow:
                    Consumer in payment-service processes offsets 0 â†’ 49.
                    It calls commit.
                    Kafka writes to _consumer_offsets:
                    key = (payment-service, orders, 0)
                    value = 50 â†’ meaning: next message to consume = offset 50.

                    If consumer crashes and restarts:
                    Broker looks up _consumer_offsets.
                    Finds entry (payment-service, orders, 0) â†’ 50.
                    Consumer resumes from offset 50.
        
      10. Kafka Delivery Semantics:
          a. At-most-once : if we do ack.acknowledge() before sout
          b. At-least-once : if we do ack.acknowledge() after sout, lets assume code fails in sout and we start consumer again, we will again get same message from topic
          c. Exactly-once: Commit + processing in same transaction, No loss, no dupes

